---
layout: archive
title: ""
permalink: /AI_for_Protein/
author_profile: true
---

<div style="width: 100%;padding-left: 3%;padding-right: 3%;background-color: rgb(241, 241, 241);">
</br>
<h2>AI for Protein</h2>
</br>
<p style="text-align:justify">
  In recent years, AI algorithms have made significant advancements in the field of protein research, particularly in the area of protein structure prediction. 
  For instance, the latest AlphaFold3 can predict the binding states between proteins and their ligands. 
  However, the relationship between protein structure and its function remains poorly characterized in terms of AI algorithms.
  We are committed to exploring the use of intelligent methods for the functional characterization of proteins, aiming to bridge the gap from structure to function. 
  For example, Protein function prediction is currently achieved by encoding its sequence or structure, where the sequence-to-function transcendence and high-quality structural data scarcity lead to obvious performance bottlenecks. 
  To fill this gap, we propose a synergistic integration approach for a function-aware domain representation, and a domain-joint contrastive learning strategy to distinguish different protein functions while aligning the modalities.
</p>
</br>
  
<div style="width: 100%;padding-left: 3%;padding-right: 3%;background-color: rgb(226, 226, 226);">
</br>
<h3>Multimodal Artificial Intelligence Large Models for Small-Molecule Drug Generation and Description</h3>
<p>Paper Link: <a href="https://arxiv.org/abs/2308.06911">https://arxiv.org/abs/2308.06911</a></p>
<img style="width: 100%;padding: 3%;" src="/images/GIT-Mol.png" />
<p style="text-align:justify">We are dedicated to the development of an innovative multimodal large model in the fields of drug discovery and molecular biology, aiming to provide new perspectives and solutions for understanding and utilizing multimodal molecular data. To this end, we have designed and implemented a multimodal AI large model capable of simultaneously processing images, graphs, and text. Through a novel cross-modal fusion approach (identifying and extracting correlations between different modalities and implementing cross-fusion of information, thereby generating a unified embedding that integrates multimodal information), our model achieves an organic interaction and efficient fusion among these three modalities. Our model has demonstrated outstanding performance across various small-molecule drug tasks, with the ability to understand and handle multimodal molecular data. It has accomplished accurate prediction of molecular properties, as well as the generation and description of molecules in response to textual prompts, showing superior performance in comparison with models of similar levels.
</p>
</br>
</div>

</br>

<div style="width: 100%;padding-left: 3%;padding-right: 3%;background-color: rgb(226, 226, 226);">
</br>
<h3>Deep Neural Network-Based Research on Small-Molecule Drugs</h3>
<p>Paper Link: <a href="https://www.biorxiv.org/content/10.1101/2023.08.15.553467v1.abstract">https://www.biorxiv.org/content/10.1101/2023.08.15.553467v1.abstract</a></p>
<img style="width: 100%;padding: 3%;" src="/images/3D-Mol.png" />
<p style="text-align:justify">By designing novel molecular representation methods and utilizing the force field computations and knowledge-base optimization features of RDKit, we generate 3D conformations of molecules from molecular topological graphs. Subsequently, we decompose the molecular conformations into point-edge graphs, bond-angle graphs, and dihedral-angle graphs to comprehensively characterize the spatial information of molecules and extract the 3D structure of small-molecule drugs. We have designed a molecular encoder based on a message-passing model and pre-trained their model using extensive unlabeled small-molecule data through a self-supervised contrastive learning approach (using molecular topological graphs). Our AI model has achieved state-of-the-art (SOTA) performance in several aspects of small-molecule drug property prediction.
</p>
<a></a>
</br>
</div>

</br>

<div style="width: 100%;padding-left: 3%;padding-right: 3%;background-color: rgb(226, 226, 226);">
</br>
<h3>RNA Splicing Prediction with Large Language model</h3>
<p>Paper Link: <a href="https://www.biorxiv.org/content/10.1101/2023.01.31.526427v2.abstract">https://www.biorxiv.org/content/10.1101/2023.01.31.526427v2.abstract</a></p>
<img style="width: 100%;padding: 3%;" src="/images/SpliceBERT.png" />
<p style="text-align:justify">RNA splicing is an important post-transcriptional process of gene expression in eukaryotic cells. Predicting RNA splicing from primary sequences can facilitate the interpretation of genomic variants. We developed a novel self-supervised pre-trained language model, SpliceBERT, to improve sequence-based RNA splicing prediction. Pre-training on pre-mRNA sequences from vertebrates enables SpliceBERT to capture evolutionary conservation information and characterize the unique property of splice sites. SpliceBERT also improves zero-shot prediction of variant effects on splicing by considering sequence context information, and achieves superior performance for predicting branchpoint in the human genome and splice sites across species. Our study highlighted the importance of pre-training genomic language models on a diverse range of species and suggested that pre-trained language models were promising for deciphering the sequence logic of RNA splicing.
</p>
</br>
</div>

</br>

</br>
</div>
