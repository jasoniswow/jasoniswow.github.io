---
layout: archive
title: ""
permalink: /Intelligent_Computing/
author_profile: true
---


<div style="width: 100%;padding-left: 3%;padding-right: 3%;background-color: rgb(241, 241, 241);">
</br>
<h2>Intelligent Computing Publication</h2>
</br>
<p style="text-align:justify">My team focus on three major research topics about intelligent computing:</p>
<p style="text-align:justify">Intelligent Time-Series Scientific Data Processing</p>
<p style="text-align:justify">Performance evaluation of large-scale intelligent computing systems</p>
<p style="text-align:justify">Intelligent Computing Standardization</p>
</br>

<div style="width: 100%;padding-left: 3%;padding-right: 3%;background-color: rgb(226, 226, 226);">
</br>
<h3>Low signal-to-noise ratio scientific data processing with deep learning</h3>
<p>Paper Link: <a href="https://www.nature.com/articles/s42005-023-01334-6">https://www.nature.com/articles/s42005-023-01334-6</a></p>
<p>Paper Link: <a href="https://link.springer.com/article/10.1007/s11467-023-1318-y">https://link.springer.com/article/10.1007/s11467-023-1318-y</a></p>
<p>Paper Link: <a href="https://arxiv.org/abs/2212.14283">https://arxiv.org/abs/2212.14283</a></p>
<p>Paper Link: <a href="https://ieeexplore.ieee.org/document/9663260">https://ieeexplore.ieee.org/document/9663260</a></p>
<img style="width: 100%;padding: 3%;" src="/images/wave.png" />
<p style="text-align:justify">Utilizing deep learning to separate signal from noise and mine information in commonly low signal-to-noise ratio (SNR) data in scientific domains: (1) To address the computational complexity bottleneck of existing Transformer models, we propose a multi-dimensional sparse self-attention method, achieving efficient training for models with up to billions of parameters. By integrating multi-level embedding features and training with a masked objective function, this method is applied to the field of signal denoising in low SNR data, achieving the best denoising effects in the current domain. (2) To overcome the limitation of existing CNNs that cannot handle ultra-long time-series signals, we propose an architecture combining dilated convolutions and recurrent neural networks. This structure enables long-term memory in the algorithm, fully learning the features of ultra-long sequences, and its application in signal detection has substantially improved detection accuracy. (3) To address the bottleneck of lengthy computational time in traditional matched filtering and Monte Carlo sampling methods, we propose a time-series signal parameter inversion algorithm based on Normalizing Flow. This deep learning algorithm can replace traditional methods, accelerating convergence through a massive number of trainable parameters. Leveraging the powerful computing capabilities of intelligent computing clusters and applying them to the field of signal parameter inversion, this method simultaneously enhances the speed and accuracy of signal parameter inversion. (4) In response to the current scarcity of open-source time-series datasets in the scientific field, we have constructed an open-source database platform and provided various categories of time-series datasets for researchers to download. Additionally, to overcome the bottlenecks of time-consuming generation, significant storage space occupation, and time-consuming data loading for long-term time-series data, we propose an adaptive parallel generation method based on in-memory databases. This solution provides ample data for large model training, accelerating the training of large-scale AI models.
</p>
</br>
</div>

</br>
  
<div style="width: 100%;padding-left: 3%;padding-right: 3%;background-color: rgb(226, 226, 226);">
</br>
<h3>Performance Evaluation and Application Platform for Large-Scale Intelligent Computing Systems</h3>
<p>Paper Link: <a href="https://ieeexplore.ieee.org/abstract/document/9430136/">https://ieeexplore.ieee.org/abstract/document/9430136/</a></p>
<img style="width: 100%;padding: 3%;" src="/images/AIPerf_2.jpg" />
<p style="text-align:justify">Performance evaluation was conducted on a large-scale intelligent computing system (consisting of 512 nodes, 4096 NPUs, 16E OPS theoretical AI computing power). By improving parameter initialization, hyperparameter optimization algorithms, and computational parallel strategies, issues such as AI accelerator idling, time-consuming model generation, and slow convergence were resolved. By developing automated deployment tools and fault tolerance mechanisms suitable for domestic intelligent computing clusters, high-speed data auto-copying required for large-scale testing was achieved. Optimization of communication strategies between large-scale nodes enabled low-latency and low-blocking task distribution among clusters. The achievements were recognized with consecutive championships for multiple years on the international AI performance AIPerf500 ranking (jointly released by ACM’s Special Interest Group on High Performance Computing in China (SIGHPC China) and China’s Big Data and Intelligent Computing Industry Alliance). Additionally, innovative research was carried out on algorithm study, model training, software optimization, etc., revolving around the design of the new generation of large-scale intelligent computing systems: (1) In the field of high-throughput data generation and processing, an in-memory database method was proposed, enabling storage-limit-breaking multi-threaded parallel data generation. (2) For large AI model training, innovative distributed training architectures were introduced, such as heuristic-guided asynchronous historical optimization algorithms, hybrid operator parallelism with decentralized communication topology for network load balancing, and model-parallel strategies for various model architectures like convolutional layer parallel splitting based on matrix outer product separation, and scalable attention mechanism parallel methods. (3) On the parallel algorithm level, improvements were made to parallel strategies targeting classic deep neural network structures, reducing communication overhead, enhancing training efficiency for large AI models, and improving model performance and convergence speed. (4) In high-performance software implementation, traditional distributed frameworks were revised, optimizing inter-node communication mechanisms to ensure training efficiency under high network latency, and enhancing the scheduling strategy of heterogeneous resources within the cluster. Surrounding the intelligent computing systems, several patents were applied for and authorized, and a large-scale intelligent computing system’s intelligent scientific research platform was constructed. The platform was optimized for stability, scalability, and compatibility, and performance improvements were made in areas such as algorithm software parallel patterns and adaptation to domestic intelligent computing systems.
</p>
</br>
</div>

</br>

<div style="width: 100%;padding-left: 3%;padding-right: 3%;background-color: rgb(226, 226, 226);">
</br>
<h3>Intelligent Computing Evaluation Standards and Standardization of Artificial Intelligence Computing Centers</h3>
<img style="width: 100%;padding: 3%;" src="/images/IC2.png" />
<p style="text-align:justify">A systematic assessment was conducted on the artificial intelligence computing center’s system architecture, performance and reliability requirements, testing methods, and more. A benchmark evaluation framework was developed for end-to-end intelligent computing that is oriented towards multimedia tasks, based on adaptive automated machine learning technology: (1) Innovatively implemented a gradual search algorithm for network structures based on convolutional blocks, utilizing a “master-slave node structure.” Slave nodes asynchronously carry out model generation and training, employing the CPU of the slave nodes to parallelly create new architectural designs, thereby enhancing the algorithm’s stability during automatic test load generation. (2) Through automated feature engineering algorithms, candidate features were autonomously created from the dataset. Several optimal values were selected from these features as expert knowledge for hyperparameter optimization, thereby enhancing the convergence speed and performance of generated loads. (3) Quantitative comparisons were realized among different hyperparameter optimization algorithms (such as random search, evolutionary algorithms), and a search for global optimum values was carried out by optimizing Bayesian methods. (4) Efficiency was achieved in the distribution of large-scale node tasks based on Kubernetes and SLURM through data parallelism and synchronous all-reduce strategies, along with the adaptive scaling of loads across machines of varying sizes. (5) An innovative application of analytical methods was used to compute floating-point operation quantities, enabling rapid and accurate predictions of the floating-point computations required in AI tasks. Additionally, the convergence of normalized scores as an evaluation metric was verified. Through a combination of the above theoretical innovations and engineering optimizations, challenges were addressed concerning hardware utilization analysis for benchmark workloads, setting costs and computation models, adaptive scalability of benchmark testing workloads, and unified performance indicators to measure AI applications. Following multiple rounds of domestic and international peer review (involving Google, Microsoft, Amazon, Tsinghua University, Peking University, Baidu, Alibaba, Tencent, Cambricon, etc.), 2 industry standards for AI computing centers were issued by the Zhongguancun Audio-Visual Industry Technology Innovation Alliance (AITISA) (T/AI 118.1—2022, T/AI 118.2—2022); an international standard for AI computing power evaluation was released by the International Telecommunication Union’s Telecommunication Standardization Sector (ITU-T SG16) (Metric and evaluation methods for AI-enabled multimedia application computing power benchmark, ITU-T F.748.18).
</p>
</br>
</div>

</br>
</br>

</div>
